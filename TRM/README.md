# 传统推荐模型（Traditional Recommendation Model）

## 协同过滤（CF）

## 1. User CF

#### 定义：基于用户之间的相似度来选取与目标用户相似的Top K个用户，然后从其中挑选目标用户没有购买过的商品进行推荐

#### 公式（1）：
$$sim(i,j)=\cos(i,j)=\frac{i\cdot j}{||i||\cdot||j||}$$ （基于余弦相似度，该方法存在用户评分偏置的情况）

#### 公式（2）：
$$sim(i,j)=\frac{\sum\limits_{p\in P}(R_{i,p}-\overline{R_{i}})(R_{j,p}-\overline{R_{j}})}{\sqrt{\sum\limits_{p \in P}(R_{i,p}-\overline{R_{i}})}\sqrt{\sum\limits_{p\in P}(R_{j,p}-\overline{R_{j}})}}$$（基于皮尔逊相关稀疏，该方法考虑了用户的平均分来缓解评分偏置）

#### 公式（3）：
$$R_{u,p}=\frac{\sum\limits_{s \in S}(W_{u,s}\cdot R_{s,p})}{\sum\limits_{s\in S}(w_{u,s})}$$（最后使用选取Top K个相似的用户来预测目标用户的打分情况）

#### 优点：结果符合直觉理解，相似的用户有相同的偏好；

#### 缺点：1.真实应用场景中，用户数目远远多于商品数目，假设用户数目为N，则需要N方的空间来存储用户之间的相似度，用户快速的增长会导致需要更大的存储空间；2.用户的历史数据往往非常稀疏，对于交互极其稀疏的用户，很难准确地找到与该用户相似的用户（大部分可能都是错误的），这使得UserCF不适用于那些正反馈获取较困难的场景（例如酒店预订、大件商品购买等低频应用）

## 2. ItemCF

#### 定义：基于物品相似度进行商品推荐，通过计算共现矩阵中物品列向量的相似度来获取相似度矩阵

#### 步骤：（1）根据用户历史行为交互数据，构建UxI矩阵（U：用户数目，I：商品数目）；（2）计算共现矩阵按列两两之间的相似度（相似度计算公式见（1）、（2）），构建IxI的物品相似性矩阵；（3）获取目标用户的正反馈列表，利用该列表在物品相似性矩阵中找出Top K个相似的物品（构成候选集）；（4）在候选集中更具相似度得分进行排序，完成推荐

#### 公式（4）：
$$R_{u,p}=\sum\limits_{h \in H}(w_{p,h}\cdot R_{u,h}),H表示目标用户正反馈集合,w_{p,h}表示物品p与物品h的相似度,R_{u,h}是目标用户对物品h的已有评分$$

### UserCF与ItemCF应用场景比较

#### UserCF基于用户相似度进行推荐，其具备较强的社交特性，用户能够快速得知与自己相似的用户最近喜欢什么，即使某个兴趣点不在自己兴趣范围之内，也有可能通过“朋友”的动态快速更新自己的推荐列表，非常适用于新闻推荐场景，因为新闻的及时性和热点性相较于用户的兴趣点更加重要；ItemCF适用于 兴趣变化较为稳定的场景（用户在一个较短周期内的兴趣是稳定的），比如电商推荐、视频推荐

### 协同过滤所面临的壁垒

#### 协同过滤是一个直观、可解释性很强的模型，但其泛化能力较弱（无法将两个物品相似这一信息推广到其他物品的相似性计算上）。这会导致一个严重的问题，热门的物品具有很强的头部效应，容易和大量物品相似；而尾部的物品由于特征稀疏，很少与其他物品产生相似性，导致被推荐的机会很少。这暴露了协同过滤的天然缺陷：推荐结果的头部效应明显，处理稀疏向量的能力弱。另外协同过滤只考虑了用户的交互信息，没有考虑用户的属性信息、商品的属性信息以及附带的社会关系等一系列特征，这会造成有效信息的遗漏

## 3. MF

#### 矩阵分解在协同过滤的共现矩阵的基础上引入了隐向量，加强了模型处理稀疏矩阵的能力

#### 定义：为每一个用户和商品生成一个隐向量，将用户和商品定义到生成的隐向量表示空间上，距离相近用户和商品表示兴趣特点接近，将距离相近的商品推荐给目标用户。可以通过矩阵分解协同过滤的共现矩阵来获取用户和商品的隐向量。该方法将大小为UxI的共现矩阵分解为UxK的用户矩阵和KxI的商品矩阵相乘的形式，其中K表示隐向量的维度；K的大小决定了隐向量表达能力的强弱。其中K取值越小，其所包含的信息量越少其泛化程度越高，反之，K值越大，其所包含的信息量就越多，其泛化程度越低。此外，K的取值还与矩阵分解的求解复杂度相关

#### 公式（5）：
$$\hat{r}_{ui}=q^{T}_{i}p_{u},其中p_{u}表示用户u在用户矩阵U中对应的行向量，q_{i}是物品i在物品矩阵V中对应的列向量$$

#### 方法：矩阵分解的三种方法（特征值分解（只能用于方阵）、奇异值分解、梯度下降）

### 3.1 奇异值分解

#### 步骤： 给定一个共现矩阵M，其大小为UxI，其一定有M=AxBxC，其中A是UxU的正交矩阵，C是一个IxI的正交矩阵，B是一个UxI的对焦矩阵，取对角矩阵B中较大的K个元素作为隐含特征，删除B中的其他维度以及A和C中对应的维度，矩阵M等价于QxWxE，其中Q，W，E分别为A，B，C中剔除不重要特征之后的矩阵，Q大小为UxK，W大小为KxK，E大小为KxI

#### 目标：该方法的目标是让原始评分与用户向量和物品向量之积的差尽量小，最大限度保存共现矩阵的原始信息

#### 公式（6）：
$$\min\limits_{q^{*},p^{*}\sum\limits_{(u,i)\in K}(r_{u,i}-q^{T}_{i}p_{u})^{2},其中K表示用户评分样本的集合$$

#### 公式（7）：
$$\min\limits_{q^{*},p^{*}\sum\limits_{(u,i)\in K}(r_{u,i}-q^{T}_{i}p_{u})^{2}+\lambda(||q_{i}||^{2}

#### +||p_{u}||^{2})，为了防止过拟合，加入正则项$$

#### 缺陷：奇异值分解需要原始矩阵是稠密的，现实场景中共现矩阵M是非常稀疏的，若使用奇异值分解，需要对一些缺失值进行填充；传统奇异值分解的计算复杂度达到了UI方的级别，这对于商品数量多的场景显然无法接受

#### Tips：正则化，其目标是让训练出来的模型更加”规则“、更加稳定，避免预测一些不稳定的”离奇“结果；过拟合就是模型在学到了数据的基本特征之后，在一些个别”噪音点“学到一些浮夸的特征的现象；将正则化项引入损失函数可以保持模型的稳定，例如，当模型的权重过大的时候，损失函数的值越大，梯度下降是朝着减少损失函数的方向发展的，因此引入正则项的目的是想在尽量不影响模型效果的情况之下，尽量减少模型的权重；权重的减少自然会让模型的输出波动减少，常见的正则方法参看：[Trick](https://github.com/QinHsiu/Trick/tree/main/similarity_distance)

#### 梯度下降：（1）确定目标函数；（2）对目标函数进行梯度下降，对目标函数（7）求偏导有：

$$q_{}$$





## Reference

#### [1] 王喆 《深度学习推荐系统》





